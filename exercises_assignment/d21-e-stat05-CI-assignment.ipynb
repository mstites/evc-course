{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448a2009",
   "metadata": {},
   "source": [
    "# Confidence Intervals and the CLT\n",
    "\n",
    "*Purpose*: When studying sampled data, we need a principled way to report our results with their uncertainties. Confidence intervals (CI) are an excellent way to summarize results, and the central limit theorem (CLT) helps us to construct these intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ac15a",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grama as gr\n",
    "DF = gr.Intention()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed219e2d",
   "metadata": {},
   "source": [
    "# Theory Fundamentals\n",
    "\n",
    "To make sense of confidence intervals, we will need a bit of background information. First up: Why are we studying confidence intervals?\n",
    "\n",
    "## Motivation: Limited data leads to error\n",
    "\n",
    "A very general phenomenon is that *limited data leads to error*. We can see this with a simple example: Estimating the mean of a known distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c0baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; run and inspect\n",
    "# Define an uncertain quantity\n",
    "mg_test = gr.marg_mom(\n",
    "    \"uniform\", \n",
    "    mean=0, \n",
    "    sd=1,\n",
    ")\n",
    "\n",
    "mg_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644e5fe",
   "metadata": {},
   "source": [
    "From the marginal summary, we can see that the mean of the distribution is `0`. However, if we have limited data then the mean of that sample will generally not be exactly zero:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2507e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; run and inspect\n",
    "gr.mean(mg_test.r(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a62773",
   "metadata": {},
   "source": [
    "Limited data leads to a form of error called *sampling error*. In practice we *almost always* have limited data, so sampling error is an unavoidable reality.\n",
    "\n",
    "Since sampling error is unavoidable, we would like to know *how much* sampling error there is in a given quantity. The central limit theorem (CLT) helps us quantify sampling error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f4a2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "Let $X \\sim \\rho$ be a random quantity with finite mean $\\mu$ and variance $\\sigma^2$. The sample mean is denoted $\\overline{X}_n$ and is defined as\n",
    "\n",
    "$$\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i.$$\n",
    "\n",
    "Under independent, repeated observations from the same distribution $X_i \\sim \\rho$, the central limit theorem states that $\\overline{X}_n$ becomes increasingly normal in its distribution as $n$ is increased, regardless of the original distribution $\\rho$. That is\n",
    "\n",
    "$$\\overline{X}_n \\stackrel{d}{\\to} N(\\mu, \\sigma^2/n)$$\n",
    "\n",
    "The CLT is useful because it gives us a way to quantify sampling error: Since the CLT holds regardless of where the data came from $X \\sim \\rho$---so long as $|\\mu| < \\infty, \\sigma < \\infty$---then we can characterize the sampling error in the sample mean $\\overline{X}_n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c26a0",
   "metadata": {},
   "source": [
    "### __q1__ Study CLT convergence behavior\n",
    "\n",
    "Adjust the sample size `n` below, run the code to inspect the results each time, then answer the questions under *observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ce7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Adjust the sample size n\n",
    "n = 1 # Sample size\n",
    "\n",
    "## NOTE: No need to edit\n",
    "# Draw a large sample\n",
    "df_sample = gr.df_make(x=mg_test.r(5000))\n",
    "\n",
    "(\n",
    "    # Compute mean within each group\n",
    "    df_sample\n",
    "    >> gr.tf_mutate(i=DF.index // n)\n",
    "    >> gr.tf_group_by(DF.i)\n",
    "    >> gr.tf_summarize(\n",
    "        x_mean=gr.mean(DF.x)\n",
    "    )\n",
    "    >> gr.tf_ungroup()\n",
    "    \n",
    "    # Visualize\n",
    "    >> gr.ggplot(gr.aes(\"x_mean\"))\n",
    "    + gr.geom_histogram(gr.aes(y=\"stat(density)\"), bins=30)\n",
    "    + gr.geom_line(\n",
    "        data=gr.df_make(x_mean=gr.linspace(-1.5, +1.5, 100))\n",
    "        >> gr.tf_mutate(\n",
    "            d=gr.marg_mom(\"norm\", mean=0, sd=1/gr.sqrt(n)).d(DF.x_mean)\n",
    "        ),\n",
    "        mapping=gr.aes(y=\"d\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908e5f1",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- Around what value of $n$ does the histogram start to look like the normal distribution (black curve)?\n",
    "  - (Your response here)\n",
    "- How does the \"width\" of the histogram change with $n$?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580edd99",
   "metadata": {},
   "source": [
    "## Confidence Intervals (CI)\n",
    "\n",
    "Since any estimated quantity has sampling error, it is common to report estimated values along with a *confidence interval*. A confidence interval (CI) is an expression of *uncertainty*; it will tend to be wider when the sampling error is larger. For the moment, let's talk about how to *compute* a confidence interval; we'll discuss how to *interpret* a confidence interval in a bit.\n",
    "\n",
    "The CLT-based confidence interval for the mean is given by\n",
    "\n",
    "$$[\\overline{X}_n - z_C \\sigma/\\sqrt{n}, \\overline{X}_n + z_C \\sigma/\\sqrt{n}]$$\n",
    "\n",
    "where $z_C$ is the relevant quantile of a standard normal distribution. This is based on capturing a specified fraction $C$ of the distribution between the interval bounds. For instance, the following code computes $z_C$ for $C = 0.99$.\n",
    "\n",
    "```{admonition} Terminology: Point Estimate vs Confidence Interval\n",
    "A single value like a sample mean $\\overline{X}_n$ is called a *point estimate*, while a confidence interval is defined by two values (the interval endpoints).\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d08f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit, this compute the relevant\n",
    "C = 0.99 # 99% confidence level\n",
    "mg_standard = gr.marg_mom(\"norm\", mean=0, sd=1)\n",
    "z_C = mg_standard.q(1 - (1 - C)/2)\n",
    "print(\"z_C = {0:4.3f}\".format(z_C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcf29fe",
   "metadata": {},
   "source": [
    "You can compute these CI bounds manually. However, the helper functions `gr.mean_lo()` and `gr.mean_up()` automate these calculations. For both functions, you can adjust the confidence level `C` with the `alpha` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605947f",
   "metadata": {},
   "source": [
    "### __q2__ Compute a confidence interval\n",
    "\n",
    "Use the helper functions `gr.mean_lo()` and `gr.mean_up()` to compute confidence limits for the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Compute confidence limits for the mean;\n",
    "df_ci = (\n",
    "    gr.df_make(x=mg_test.r(10))\n",
    "    >> gr.tf_summarize(\n",
    "\n",
    "        # x_lo=???\n",
    "        x_mean=gr.mean(DF.x),\n",
    "\n",
    "        # x_up=???\n",
    "    )\n",
    ")\n",
    "\n",
    "## NOTE: Use this to check your work\n",
    "assert \\\n",
    "    \"x_lo\" in df_ci.columns, \\\n",
    "    \"You must compute x_lo\"\n",
    "assert \\\n",
    "    \"x_up\" in df_ci.columns, \\\n",
    "    \"You must compute x_up\"\n",
    "assert \\\n",
    "    df_ci.x_lo[0] < df_ci.x_mean[0], \\\n",
    "    \"x_lo should be smaller than x_mean\"\n",
    "assert \\\n",
    "    df_ci.x_mean[0] < df_ci.x_up[0], \\\n",
    "    \"x_mean should be smaller than x_up\"\n",
    "\n",
    "df_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06c630",
   "metadata": {},
   "source": [
    "### __q3__ Study the CI width\n",
    "\n",
    "The following code creates confidence intervals for samples of different sizes $n$. Run the following code, inspect the results, and answer the questions under *observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6521a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_sweep = gr.df_grid()\n",
    "for i in [10, 20, 40, 80, 160]:\n",
    "    df_sweep = (\n",
    "        df_sweep\n",
    "        >> gr.tf_bind_rows(\n",
    "            gr.df_make(x=mg_test.r(i))\n",
    "            >> gr.tf_summarize(\n",
    "                x_lo=gr.mean_lo(DF.x),\n",
    "                x_up=gr.mean_up(DF.x),\n",
    "            )\n",
    "            >> gr.tf_mutate(\n",
    "                ci_width=DF.x_up - DF.x_lo,\n",
    "                n=i,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "(\n",
    "    df_sweep\n",
    "    >> gr.ggplot(gr.aes(\"n\", \"ci_width\"))\n",
    "    + gr.geom_line()\n",
    "    + gr.labs(\n",
    "        x=\"Sample Size n\",\n",
    "        y=\"CI Width\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a419c65",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- Compare the CI width behavior (figure immediately above) with the definition given earlier. What explains the curve above?\n",
    "  - (Your response here)\n",
    "- To halve the CI width, how would you need to change the sample size?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1401d",
   "metadata": {},
   "source": [
    "# Interpreting Confidence Intervals\n",
    "\n",
    "Confidence intervals are constructed in a way that seeks to *include* the true value that we seek to estimate. However, the nature of uncertainty means we need to exercise care when interpreting a confidence interval.\n",
    "\n",
    "## Golden Rule of Confidence Intervals\n",
    "\n",
    "```{admonition} The \"Golden Rule\" for interpreting Confidence Intervals\n",
    "When interpreting a confidence interval, we should assume the true value could be anywhere inside the interval.\n",
    "```\n",
    "\n",
    "Interpreting a CI as a range of possibilities helps us \"hedge our bets\". The point estimate may suggest a favorable conclusion, but if the CI is wide, then an unfavorable conclusion may also be possible.\n",
    "\n",
    "If a CI does not allow you to exclude an unfavorable possibility, it is an indication that you should gather another *larger* sample in order to study the same problem with greater statistical precision. You'll practice these ideas in the following exercises.\n",
    "\n",
    "```{admonition} Confidence in *procedure*, not in the interval\n",
    ":class: warning\n",
    "A subtle point about confidence intervals is that our confidence is in the long-run properties of constructing *many* intervals, not in any *single* interval. Any individual interval either does or does not include its true value: the \"probability\" for a single interval is either `0` or `1`.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3fe93",
   "metadata": {},
   "source": [
    "### __q4__ Interpret these intervals\n",
    "\n",
    "Study the following intervals, answer the questions under *observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; run and inspect\n",
    "(\n",
    "    gr.df_make(\n",
    "        case=[\"A\", \"B\", \"C\", \"D\"],\n",
    "        x_lo=[ -1,   1,  -2,  -4],\n",
    "        x_mu=[  1, 1.5,  -1,  -3],\n",
    "        x_up=[  4,   2,   1,  -1],\n",
    "    )\n",
    "    >> gr.ggplot(gr.aes(\"case\"))\n",
    "    + gr.geom_errorbar(gr.aes(ymin=\"x_lo\", ymax=\"x_up\"))\n",
    "    + gr.geom_point(gr.aes(y=\"x_mu\"))\n",
    "    + gr.geom_hline(yintercept=0, linetype=\"dashed\")\n",
    "    + gr.coord_flip()\n",
    "    + gr.labs(\n",
    "        y=\"Value\",\n",
    "        x=\"Case\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449c205",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- Is Case A greater than zero?\n",
    "  - (Your response here)\n",
    "- Is Case B greater than zero?\n",
    "  - (Your response here)\n",
    "- Is Case C less than zero?\n",
    "  - (Your response here)\n",
    "- Is Case D less than zero?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b28b27",
   "metadata": {},
   "source": [
    "# Probability Estimation\n",
    "\n",
    "Remember that we learned in the previous exercise [stat04-distributions](https://zdelrosario.github.io/evc-course/exercises_solution/d19-e-stat04-distributions-solution.html#probability) that probability can be expressed as a mean (expectation). Thus, we can apply the same ideas about confidence intervals when estimating probabilities.\n",
    "\n",
    "To demonstrate, let's study a simple structural model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grama.models import make_cantilever_beam\n",
    "md_beam = make_cantilever_beam()\n",
    "md_beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a49829a",
   "metadata": {},
   "source": [
    "This model has two outputs that model failure modes: `g_stress <= 0` when the beam exceeds its maximum stress, and `g_disp <= 0` when the tip deflection is beyond its designed limit. We can assess the probability of failure for both failure modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit\n",
    "(\n",
    "    md_beam\n",
    "    >> gr.ev_sample(n=100, df_det=\"nom\")\n",
    "    >> gr.tf_summarize(\n",
    "        pof_stress=gr.pr(DF.g_stress <= 0),\n",
    "        pof_disp=gr.pr(DF.g_disp <= 0),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f765ef3",
   "metadata": {},
   "source": [
    "Like the mean CI helpers, grama provides `gr.pr_lo()` and `gr.pr_up()` to easily compute confidence intervals on a probability:\n",
    "\n",
    "```{admonition} The probability CI helpers are different\n",
    "Note that the probability CI helpers `gr.pr_lo()` and `gr.pr_up()` are different from `gr.mean_lo()` and `gr.mean_up()`; namely, they only return values between `[0, 1]`. Make sure to only use the probability helpers when estimating probabilities!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66858bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    md_beam\n",
    "    >> gr.ev_sample(n=100, df_det=\"nom\")\n",
    "    >> gr.tf_summarize(\n",
    "        pof_lo=gr.pr_lo(DF.g_stress <= 0),\n",
    "        pof=gr.pr(DF.g_stress <= 0),\n",
    "        pof_up=gr.pr_up(DF.g_stress <= 0),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe22539",
   "metadata": {},
   "source": [
    "This is an *extremely* wide confidence interval for the probability of failure; I get a value of `pof` anywhere between `0.005` and `0.30`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbc5bd",
   "metadata": {},
   "source": [
    "### __q5__ Adjust the sample size\n",
    "\n",
    "Adjust the sample size `n` to achieve a CI width less than `0.1`.  Answer the questions under *observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Adjust the sample size\n",
    "df_pof = (\n",
    "    md_beam\n",
    "    >> gr.ev_sample(\n",
    "        # n=20, \n",
    "\n",
    "        df_det=\"nom\",\n",
    "    )\n",
    "    >> gr.tf_summarize(\n",
    "        pof_lo=gr.pr_lo(DF.g_stress <= 0),\n",
    "        pof=gr.pr(DF.g_stress <= 0),\n",
    "        pof_up=gr.pr_up(DF.g_stress <= 0),\n",
    "    )\n",
    ")\n",
    "\n",
    "## NOTE: No need to edit; use to check your work\n",
    "assert \\\n",
    "    abs(df_pof.pof_up[0] - df_pof.pof_lo[0]) < 0.1, \\\n",
    "    \"CI is too wide; increase `n`\"\n",
    "\n",
    "df_pof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7c3f3",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- Is the `pof` smaller than `0.04`?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727bb7e",
   "metadata": {},
   "source": [
    "### __q6__ Design for small failure rate\n",
    "\n",
    "Adjust the deterministic variables `w` and `t` in order to achieve a probability of failure due to stress less than `0.001`.\n",
    "\n",
    "*Hint*: You adjusted the sample size `n` in the previous exercise. Make sure to use that learning when tackling this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Adjust the deterministic variables `w, t` to achieve a probability\n",
    "## of failure less than 0.001\n",
    "(\n",
    "    md_beam\n",
    "    >> gr.ev_sample(\n",
    "        # n=20,\n",
    "        # df_det=gr.df_make(w=2.0, t=2.0),\n",
    "\n",
    "    )\n",
    "    >> gr.tf_summarize(\n",
    "        pof_lo=gr.pr_lo(DF.g_stress <= 0),\n",
    "        pof=gr.pr(DF.g_stress <= 0),       # I find that pof ~= 0.0\n",
    "        pof_up=gr.pr_up(DF.g_stress <= 0), # pof_up must be < 0.001\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a0df0",
   "metadata": {},
   "source": [
    "# Real vs Error\n",
    "\n",
    "In [stat02-source](https://zdelrosario.github.io/evc-course/exercises_solution/d08-e-stat02-source-solution.html#) we learned that we should treat real and erroneous sources of variability differently. Let's apply that thinking to the beam example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772ef66",
   "metadata": {},
   "source": [
    "### __q7__ Selecting the proper statistical technique\n",
    "\n",
    "Suppose the variability exhibited by the model `md_beam` is real. Study the results from the simulation below, and answer the questions under *observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: No need to edit; run and inspect the results\n",
    "(\n",
    "    md_beam\n",
    "    >> gr.ev_sample(\n",
    "        n=1e4, \n",
    "        df_det=\"nom\",\n",
    "    )\n",
    "    >> gr.tf_summarize(\n",
    "        g_lo=gr.mean_lo(DF.g_stress),\n",
    "        g_mean=gr.mean(DF.g_stress),\n",
    "        g_up=gr.mean_up(DF.g_stress),\n",
    "\n",
    "        pof_lo=gr.pr_lo(DF.g_stress <= 0),\n",
    "        pof=gr.pr(DF.g_stress <= 0),\n",
    "        pof_up=gr.pr_up(DF.g_stress <= 0),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23e8f6",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- Is the variability in `g_stress` real or erroneous?\n",
    "  - (Your response here)\n",
    "- Is the mean of `g_stress` greater than zero? How do you know?\n",
    "  - (Your response here)\n",
    "- What is the probability of failure due to stress?\n",
    "  - (Your response here)\n",
    "- Which of the two statistical techniques---computing the mean or computing a probability of failure---better characterizes the safety of this system? Why?\n",
    "  - (Your response here)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
